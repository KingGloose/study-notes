# 1 基本介绍

## 1.1 向量表征

在人工智能领域，向量表征（Vector Representation）是核心概念之一。通过将文本、图像、声音、行为甚至复杂关系转化为高维向量（Embedding），AI系统能够以数学方式理解和处理现实世界中的复杂信息。这种表征方式为机器学习模型提供了统一的“语言”。

![[00 assets/c0a25dcfc7f3209afcd11fb821a0c9bc_MD5.jpeg]]

![[00 assets/b36bf1020d68a238095d66960481ff2c_MD5.jpeg]]

![[00 assets/986e40411e086651a311c0af13a68c8e_MD5.jpeg]]

## 1.2 向量概念

向量是一种有大小和方向的数学对象。它可以表示为从一个点到另一个点的有向线段。例如，二维空间中的向量可以表示为 $(x,y)$，表示从原点 $(0,0)$ 到点 $(x,y)$ 的有向线段。

![[00 assets/c83d0e6cd822de1db98093e92a54d10d_MD5.jpeg]]

以此类推，我可以用一组坐标 $(x_0, x_1, \ldots, x_{N-1})$ 表示一个 $N$ 维空间中的向量，$N$ 叫向量的维度。

## 1.3 文本向量

![[00 assets/db9b3346625a00602217fb3533b316cb_MD5.jpeg]]

![[00 assets/65cfbc8b2d08376e83ee510ca7851950_MD5.jpeg]]

**余弦相似度 (a · b) / (||a|| × ||b||**)
1、点积 (dot product)： a · b = Σ(ai × bi)
- 对应元素相乘后求和
- 例如：a=[1,2,3], b=[4,5,6] → dot(a,b) = 1×4 + 2×5 + 3×6 = 32

2、向量模长 (norm)： ||a|| = √(Σai²)
- 向量的欧几里得长度
- 例如：||a|| = √(1² + 2² + 3²) = √14

3、最终计算: `cos_sim = 32 / (√14 × √77) ≈ 0.974`

**欧几里得距离 √(Σ(ai - bi)²)**

1、向量差： x = a - b
- 对应元素相减
- 例如：a=[1,2,3], b=[4,5,6] → x=[-3,-3,-3]

 2、平方和： Σ(xi²)
- 差值的平方和
- 例如：(-3)² + (-3)² + (-3)² = 27

3、 开平方根：√27 ≈ 5.196

# 2 Embedding

## 2.1 基本使用

目前演示使用的 `硅基流动` 的 `model api` 来实现的针对数据向量化的计算，如果值越小就越接近
![[00 assets/13030570bec5d64c2068b35aca32ce37_MD5.jpeg]]

## 2.2 基本介绍

![[00 assets/df712733707a85fd4cd799d1e31c6c31_MD5.jpeg]]

![[00 assets/df3a1e958130f17dd515f927f6f5f6e1_MD5.jpeg]]

下面就是针对不同的需求，选择不同的 Embedding 模型来使用，目前可以进这个网站来看排名：[Hugging Face – The AI community building the future.](https://huggingface.co/)

![[00 assets/073a516fd97841ad97e94dc1bdcd6d8d_MD5.jpeg]]

![[00 assets/ba81cf28d5fdf2b63ff8cab206bef3cd_MD5.jpeg]]

# 3 向量数据库

## 3.1 基本介绍

![[00 assets/5b076931866d2646552bd4b09159fa24_MD5.jpeg]]

![[00 assets/9c26328ea731d983f74389410301ea85_MD5.jpeg]]

![[00 assets/6670007312dd68325d25fb175277c513_MD5.jpeg]]

简单来说 milvus 可以支持一些重量级别的商业开发使用，如果是轻量级的话，使用 Chroma 向量数据库就行
![[00 assets/0cd3918a605517b54e81e4010d5bdf36_MD5.jpeg]]

## 3.2 Chroma

### 3.2.1 基本介绍

![[00 assets/bf990f603f4f9683775c430d4bf26ba8_MD5.jpeg]]

### 3.2.2 基本使用

1、首先是安装，针对 nodejs 版本，windows 暂时不支持 x64 的版本，只能 arm64，这个时候安装 python 就行
2、安装 python 可以参考 [[Python 切换版本]] 
![[00 assets/f9f24e8c4a2dca0aa8e75299b1479122_MD5.jpeg]]

3、我们安装完 python 之后，正常启动即可
![[00 assets/7f8f21940862a9ac48c5d91b91083947_MD5.jpeg]]

如果你使用的 path 的话，可以看到，本质其实就是 `sqlite3` 来做的数据存储
![[00 assets/fb93c2f748e6638fd2951bd84066aad6_MD5.jpeg]]

4、因为 Chroma 内置了默认的 embedding 嵌入模型，但是我们这里使用自定义的模型，按照如下的方式编写即可，具体可以参考如下的内容：[Embedding Functions - Chroma Docs](https://docs.trychroma.com/docs/embeddings/embedding-functions?lang=typescript)
![[00 assets/5f47dac5a1f97c3a7603b8be70af9959_MD5.jpeg]]

5、编写的代码基本和数据库基本一致，具体使用查看文档即可
![[00 assets/ba689e2ad1215eda5c70854ed2718489_MD5.jpeg]]

其实在初始化的时候，也可以使用不同的算法，以及文本相似度如何计算，比如这里的 “余弦相似度”
![[00 assets/c6461a2995100c819ddd100883b9048d_MD5.jpeg]]

最终执行的效果如下
![[00 assets/c050cb67bce6a1d9694b034180d07a5e_MD5.jpeg]]

## 3.3 Milvus

待测试

# 4 RAG

## 4.1 基本介绍

![[00 assets/698057c09a47de79ecba9e3d17fdbb65_MD5.jpeg]]

> RAG 的优势是什么？

- **解决知识时效性问题**：大模型的训练数据通常是静态的，无法涵盖最新信息，而RAG可以检索外部知识库实时更新信息
- **减少模型幻觉**：通过引入外部知识，RAG能够减少模型生成虚假或不准确内容的可能性
- **提升专业领域回答质量**：RAG能够结合垂直领域的专业知识库，生成更具专业深度的回答
- **生成内容的溯源（可解释性）**

> RAG 核心原理

**Step1：数据预处理，构建索引库**
- 知识库构建：收集并整理文档、网页、数据库等多源数据，构建外部知识库
- 文档分块：将文档切分为适当大小的片段（chunks），以便后续检索。分块策略需要在语义完整性与检索效率之间取得平衡
- 向量化处理：使用嵌入模型（如BGE、M3E、Chinese-Alpaca-2等）将文本块转换为向量，并存储在向量数据库中

**Step2：检索阶段**
- 查询处理：将用户输入的问题转换为向量，并在向量数据库中进行相似度检索，找到最相关的文本片段
- 重排序：对检索结果进行相关性排序，选择最相关的片段作为生成阶段的输入（这里的排序是最大的，内部还是比较复杂的）

**Step3：生成阶段**
- 上下文组装：将检索到的文本片段与用户问题结合，形成增强的上下文输入
- 生成回答：大语言模型基于增强的上下文生成最终回答

![[00 assets/4c0a6d006e6a3db41c7989b82ad66e93_MD5.jpeg]]

## 4.2 NativeRAG

NativeRAG的步骤：
- Indexing => 如何更好地把知识存起来。
- Retrieval => 如何在大量的知识中，找到一小部分有用的，给到模型参考。
- Generation => 如何结合用户的提问和检索到的知识，让模型生成有用的答案。

![[00 assets/3f3c6087f9c729d597d1110306896b1b_MD5.jpeg]]

## 4.3 LangChain

 ### 4.3.1 基本介绍

![[00 assets/c01ff178a530924ffb18058ba4f26fca_MD5.jpeg]]
![[00 assets/69b13762abb79652522c30243546e7df_MD5.jpeg]]

### 4.3.2 基本使用






## 4.4 提升质量

### 4.4.1 数据准备阶段

>常见问题

**数据质量差**：企业大部分数据（尤其是非结构化数据）缺乏良好的数据治理，未经标记/评估的非结构化数据可能包含敏感、过时、矛盾或不正确的信息。
**多模态信息**：提取、定义和理解文档中的不同内容元素，如标题、配色方案、图像和标签等存在挑战。
**复杂的PDF提取**：PDF是为人类阅读而设计的，机器解析起来非常复杂。


>如何提升数据准备阶段的质量？

构建完整的数据准备流程，采用智能文档技术

**（1）构建完整的数据准备流程**

**数据评估与分类**
- 数据审计：全面审查现有数据，识别敏感、过时、矛盾或不准确的信息。
- 数据分类：按类型、来源、敏感性和重要性对数据进行分类，便于后续处理。

**数据清洗**
- 去重：删除重复数据
- 纠错：修正格式错误、拼写错误等
- 更新：替换过时信息，确保数据时效性
- 一致性检查：解决数据矛盾，确保逻辑一致

**敏感信息处理**
- 识别敏感数据：使用工具或正则表达式识别敏感信息，如个人身份信息
- 脱敏或加密：对敏感数据进行脱敏处理，确保合规。

**数据标记与标注**
- 元数据标记：为数据添加元数据，如来源、创建时间等
- 内容标注：对非结构化数据进行标注，便于后续检索和分析

**数据治理框架**
- 制定政策：明确数据管理、访问控制和更新流程
- 责任分配：指定数据治理负责人，确保政策执行
- 监控与审计：定期监控数据质量，进行审计



**（2）智能文档技术**

1、针对 PDF 的处理，也可以使用 RAG Flow，他是基于 OCR 识别来实现的，精度也不错

2、也可以使用商业的方案来实现
阿里文档智能：https://www.aliyun.com/product/ai/docmind?spm=a2c4g.11174283.0.0.bfe667a8tIVMdG
微软 LayoutLMv3：https://www.microsoft.com/en-us/research/articles/layoutlmv3/
![[00 assets/9d3673b5660cc641866b6d74c5a0d6b8_MD5.jpeg]]


### 4.4.2 知识检索阶段

>常见问题

**内容缺失**：当检索过程缺少关键内容时，系统会提供不完整、碎片化的答案 => 降低RAG的质量
**错过排名靠前的文档**：用户查询相关的文档时被检索到，但相关性极低，导致答案不能满足用户需求，这是因为在检索过程中，用户通过主观判断决定检索“文档数量”。理论上所有文档都要被排序并考虑进一步处理，但在实践中，通常只有排名top k的文档才会被召回，而k值需要根据经验确定。
**不在上下文中**：从数据库中检索出包含答案的文档，但未能包含在生成答案的上下文中。这种情况通常发生在返回大量文件时，需要进行整合以选择最相关的信息。


>如何提升知识检索阶段的质量？

通过查询转换澄清用户意图：明确用户意图，提高检索准确性。采用混合检索和重排策略：确保最相关的文档被优先处理，生成更准确的答案。

**（1）通过查询转换澄清用户意图**

**场景**：用户询问 “如何申请信用卡？”
**问题**：用户意图可能模糊，例如不清楚是申请流程、所需材料还是资格条件。
**解决方法**：通过查询转换明确用户意图。
**实现步骤**：
- 意图识别：使用自然语言处理技术识别用户意图。例如，识别用户是想了解流程、材料还是资格。
- 查询扩展：根据识别结果扩展查询。例如：
	- 如果用户想了解流程，查询扩展为“信用卡申请的具体步骤”
	- 如果用户想了解材料，查询扩展为“申请信用卡需要哪些材料”
	- 如果用户想了解资格，查询扩展为“申请信用卡的资格条件”
- 检索：使用扩展后的查询检索相关文档
- 示例：
	- 用户输入：“如何申请信用卡？”
	- 系统识别意图为 `流程`，扩展查询为 `信用卡申请的具体步骤`
	- 检索结果包含详细的申请步骤文档，系统生成准确答案


**（2）混合检索和重排策略**

**场景**：用户询问“信用卡年费是多少？”
**问题**：直接检索可能返回大量文档，部分相关但排名低，导致答案不准确。
**解决方法**：采用混合检索和重排策略。
**步骤**：
- **混合检索**：结合关键词检索和语义检索。比如：关键词检索：“信用卡年费”。
- **语义检索**：使用嵌入模型检索与“信用卡年费”语义相近的文档。
- **重排**：对检索结果进行重排。
- **生成答案**：从重排后的文档中生成答案。
**示例**：
- 用户输入：“信用卡年费是多少？”
- 系统进行混合检索，结合关键词和语义检索。
- 重排后，最相关的文档（如“信用卡年费政策”）排名靠前。
- 系统生成准确答案：“信用卡年费根据卡类型不同，普通卡年费为100元，金卡为300元，白金卡为1000元。”


### 4.4.3 答案生成阶段

>常见问题

**未提取**：答案与所提供的上下文相符，但大语言模型却无法准确提取。这种情况通常发生在上下文中存在过多噪音或相互冲突的信息时。
**不完整**：尽管能够利用上下文生成答案，但信息缺失会导致对用户查询的答复不完整。格式错误：当prompt中的附加指令格式不正确时，大语言模型可能误解或曲解这些指令，从而导致错误的答案。
**幻觉**：大模型可能会产生误导性或虚假性信息。


>如何提升答案生成阶段的质量？

改进提示词模板，实施动态防护栏

**（1）改进提示词模板**

|         场景         |          原始提示词           |                     改进后的提示词                     |
| :----------------: | :----------------------: | :---------------------------------------------: |
|   用户询问“如何申请信用卡？”   |  “根据以下上下文回答问题：如何申请信用卡？”  |     “根据以下上下文，提取与申请信用卡相关的具体步骤和所需材料：如何申请信用卡？”     |
| 用户询问“信用卡的年费是多少？”   | “根据以下上下文回答问题：信用卡的年费是多少？” | “根据以下上下文，详细列出不同信用卡的年费信息，并说明是否有减免政策：信用卡的年费是多少？”  |
|   用户询问“什么是零存整取？”   |  “根据以下上下文回答问题：什么是零存整取？”  | “根据以下上下文，准确解释零存整取的定义、特点和适用人群，确保信息真实可靠：什么是零存整取？” |

**如何对原有的提示词进行优化？**

可以通过 `DeepSeek-R1` 或 `QWQ` 的推理链，对提示词进行优化：
- 信息提取：从原始提示词中提取关键信息。
- 需求分析：分析用户的需求，明确用户希望获取的具体信息。
- 提示词优化：根据需求分析的结果，优化提示词，使其更具体、更符合用户的需求。


**（2）实施动态防护栏**

动态防护栏（Dynamic Guardrails）是一种在生成式AI系统中用于实时监控和调整模型输出的机制，旨在确保生成的内容符合预期、准确且安全。它通过设置规则、约束和反馈机制，动态地干预模型的生成过程，避免生成错误、不完整、不符合格式要求或含有虚假信息（幻觉）的内容。

在RAG系统中，动态防护栏的作用尤为重要，因为它可以帮助解决以下问题：
- **未提取**：确保模型从上下文中提取了正确的信息。
- **不完整**：确保生成的答案覆盖了所有必要的信息。
- **格式错误**：确保生成的答案符合指定的格式要求。
- **幻觉**：防止模型生成与上下文无关或虚假的信息。

**场景1：防止未提取**

用户问题：“如何申请信用卡？”
- 上下文：包含申请信用卡的步骤和所需材料。
- 动态防护栏规则：检查生成的答案是否包含“步骤”和“材料”。如果缺失，提示模型重新生成。
- 示例：
    - 错误输出：“申请信用卡需要提供一些材料。”
    - 防护栏触发：检测到未提取具体步骤，提示模型补充。

**场景2：防止不完整**

用户问题：“信用卡的年费是多少？”
- 上下文：包含不同信用卡的年费信息。
- 动态防护栏规则：检查生成的答案是否列出所有信用卡的年费。如果缺失，提示模型补充。
- 示例：
    - 错误输出：“信用卡A的年费是100元。”
    - 防护栏触发：检测到未列出所有信用卡的年费，提示模型补充。

**场景3：防止幻觉**

用户问题：“什么是零存整取？”
- 上下文：包含零存整取的定义和特点。
- 动态防护栏规则：检查生成的答案是否与上下文一致。如果不一致，提示模型重新生成。
- 示例：
    - 错误输出：“零存整取是一种贷款产品。
    - 防护栏触发：检测到与上下文不一致，提示模型重新生成。

**如何实现动态防护栏技术？**

事实性校验规则，在生成阶段，设置规则验证生成内容是否与检索到的知识片段一致。例如，可以使用参考文献验证机制，确保生成内容有可靠来源支持，避免输出矛盾或不合理的回答。

**如何制定事实性校验规则？**

当业务逻辑明确且规则较为固定时，可以人为定义一组规则，比如：
规则1：生成的答案必须包含检索到的知识片段中的关键实体（如“年费”、“利率”）。
规则2：生成的答案必须符合指定的格式（如步骤列表、表格等）。
实施方法：
- 使用正则表达式或关键词匹配来检查生成内容是否符合规则。
- 例如，检查生成内容是否包含“年费”这一关键词，或者是否符合步骤格式（如“1. 登录；2. 设置”）。


### 4.4.4 不同阶段提高

**数据准备环节**，阿里云考虑到文档具有多层标题属性且不同标题之间存在关联性，提出多粒度知识提取方案，按照不同标题级别对文档进行拆分，然后基于Qwen14b模型和RefGPT训练了一个面向知识提取任务的专属模型，对各个粒度的chunk进行知识提取和组合，并通过去重和降噪的过程保证知识不丢失、不冗余。最终将文档知识提取成多个事实型对话，提升检索效果；

**知识检索环节**，哈啰出行采用多路召回的方式，主要是向量召回和搜索召回。其中，向量召回使用了两类，一类是大模型的向量、另一类是传统深度模型向量；搜索召回也是多链路的，包括关键词、ngram等。通过多路召回的方式，可以达到较高的召回查全率。

**答案生成环节**，中国移动为了解决事实性不足或逻辑缺失，采用FoRAG两阶段生成策略，首先生成大纲，然后基于大纲扩展生成最终答案。


# 5 RAG 高级

## 5.1 RAG 树

![[00 assets/ae8d214be214455dd16e88487c902f9a_MD5.jpeg]]

- RAG研究的技术树主要涉及预训练（Pre-training）、微调（Fine-tuning）和推理（Inference）等阶段。
- 随着LLM的出现，RAG的研究最初侧重于利用LLMs强大的上下文学习能力，主要集中在推理阶段。
- 随后的研究进一步深入，逐渐与LLMs的微调阶段更加融合。研究人员也在探索通过检索增强技术来提升预训练阶段的语言模型性能。

**参考文章**：https://www.promptingguide.ai/research/rag


## 5.2 RAFT 方法

RAFT方法（Retrieval Augmented Fine Tuning）

RAFT: Adapting Language Model to Domain Specific RAG, 2024 https://arxiv.org/pdf/2403.10131

如何最好地准备考试？
- 基于微调的方法通过“学习”来实现“记忆”输入文档或回答练习题而不参考文档。
- 或者，基于上下文检索的方法未能利用固定领域所提供的学习机会，相当于参加开卷考试但没有事先复习。
- 相比之下，我们的方法RAFT利用了微调与问答对，并在一个模拟的不完美检索环境中参考文档——从而有效地为开卷考试环境做准备。
![[00 assets/e17043367143596221b67c604553206a_MD5.jpeg]]
  
  让LLMs从一组正面和干扰文档中读取解决方案，这与标准的RAG设置形成对比，因为在标准的RAG设置中，模型是基于检索器输出进行训练的，这包含了记忆和阅读的混合体。在测试时，所有方法都遵循标准的RAG设置，即提供上下文中排名前k的检索文档。

![[00 assets/1dbbe7ef6f18abe1697d2bfb086d1a32_MD5.jpeg]]

微调数据集准备样例：

![[00 assets/2b9a74278cd5a6969464b999e09a1a10_MD5.jpeg]]

RAFT在所有专业领域的RAG性能上有所提升（在PubMed、HotPot、HuggingFace、Torch Hub和TensorflowHub等多个领域），领域特定的微调提高了基础模型的性能，RAFT无论是在有RAG的情况下还是没有RAG的情况下，都持续优于现有的领域特定微调方法。这表明了需要在上下文中训练模型。

![[00 assets/d49098c9db6e161b63ed56b3aefa7028_MD5.jpeg]]

**总结：**

RAFT方法（Retrieval Augmented Fine Tuning）：
- 适应特定领域的LLMs对于许多新兴应用至关重要，但如何有效融入信息仍是一个开放问题。
- RAFT结合了检索增强生成（RAG）和监督微调（SFT），从而提高模型在特定领域内回答问题的能力。
- 训练模型识别并忽略那些不能帮助回答问题的干扰文档，只关注和引用相关的文档。
- 通过在训练中引入干扰文档，提高模型对干扰信息的鲁棒性，使其在测试时能更好地处理检索到的文档。

训练示例：https://github.com/lumpenspace/raft


## 5.3 RAG高效召回方法

### 5.3.1 合理设置TOP_K

```python
docs = knowledgeBase.similarity_search(query, k=10)
```


### 5.3.2 改进索引算法

**知识图谱：** 利用知识图谱中的语义信息和实体关系，增强对查询和文档的理解，提升召回的相关性


### 5.3.3 引入重排序

**重排序模型：** 对召回结果进行重排，提升问题和文档的相关性。常见的重排序模型有 `BGE-Rerank` 和 `Cohere Rerank`。
- 场景：用户查询“如何提高深度学习模型的训练效率？”
- 召回结果：初步召回10篇文档，其中包含与“深度学习”、“训练效率”相关的文章。
- 重排序：BGE-Rerank对召回的10篇文档进行重新排序，将与“训练效率”最相关的文档（如“优化深度学习训练的技巧”）排在最前面，而将相关性较低的文档（如“深度学习基础理论”）排在后面。

**混合检索：** 结合向量检索和关键词检索的优势，通过重排序模型对结果进行归一化处理，提升召回质量。


### 5.3.4 优化查询扩展

**相似语义改写：** 使用大模型将用户查询改写成多个语义相近的查询，提升召回多样性。
- 例如，LangChain的 `MultiQueryRetriever` 支持多查询召回，再进行回答问题。

```python
import os
from langchain.retrievers import MultiQueryRetriever
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.llms import Tongyi

# 初始化大语言模型
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
llm = Tongyi(
    model_name="qwen-max", 
    dashscope_api_key=DASHSCOPE_API_KEY
)

# 创建嵌入模型
embeddings = DashScopeEmbeddings(
    model="text-embedding-v3",
    dashscope_api_key=DASHSCOPE_API_KEY
)

# 加载向量数据库，添加allow_dangerous_deserialization=True参数以允许反序列化
vectorstore = FAISS.load_local("./vector_db", embeddings, allow_dangerous_deserialization=True)

# 创建MultiQueryRetriever
retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm
)

# 示例查询
query = "客户经理的考核标准是什么？"
# 执行查询
results = retriever.get_relevant_documents(query)

# 打印结果
print(f"查询: {query}")
print(f"找到 {len(results)} 个相关文档:")
for i, doc in enumerate(results):
    print(f"\n文档 {i+1}:")
    print(doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content)
```

### 5.3.5 双向改写

将查询改写成文档（Query2Doc）或为文档生成查询（Doc2Query），缓解短文本向量化效果差的问题

**1. Query2Doc：将查询改写成文档**
- 用户查询：“如何提高深度学习模型的训练效率？”
- Query2Doc 改写：
    - 原始查询较短，可能无法充分表达用户意图。
    - 通过 Query2Doc 生成一段扩展文档：
        提高深度学习模型的训练效率可以从以下几个方面入手：
        1. 使用更高效的优化算法，如AdamW或LAMB。
        2. 采用混合精度训练（Mixed Precision Training），减少显存占用并加速计算。
        3. 使用分布式训练技术，如数据并行或模型并行。
        4. 对数据进行预处理和增强，减少训练时的冗余计算。
        5. 调整学习率调度策略，避免训练过程中的震荡。

**2. Doc2Query：为文档生成关联查询**
- 文档内容：
    本文介绍了深度学习模型训练中的优化技巧，包括：
    1. 使用AdamW优化器替代传统的SGD。
    2. 采用混合精度训练，减少显存占用。
    3. 使用分布式训练技术加速大规模模型的训练……
- 通过 Doc2Query 生成一组可能的查询：
    1. 如何选择深度学习模型的优化器？
    2. 混合精度训练有哪些优势？
    3. 分布式训练技术如何加速深度学习？
    4. 如何减少深度学习训练中的显存占用？
    5. 深度学习模型训练的最佳实践是什么？


### 5.3.6 索引扩展

- **离散索引扩展：** 使用关键词抽取、实体识别等技术生成离散索引，与向量检索互补，提升召回准确性。
- **连续索引扩展：** 结合多种向量模型（如OpenAI的Ada、智源的BGE）进行多路召回，取长补短。
- **混合索引召回：** 将BM25等离散索引与向量索引结合，通过Ensemble Retriever实现混合召回，提升召回多样性

#### 5.3.6.1 离散索引

![[00 assets/e3f41b67fadc7e466535fc58c2c5b821_MD5.jpeg]]


#### 5.3.6.2 混合索引召回

![[00 assets/e17b1fdb9a0922c7e51cc44e3938d307_MD5.jpeg]]


#### 5.3.6.3 Small-to-Big

![[00 assets/77c18916051d5bc2cf46d138e7851570_MD5.jpeg]]

![[00 assets/8cf6503d0fb0688fca65ed1eec54cf6c_MD5.jpeg]]



## 5.4 基本使用

